# 코드
~~~python
import requests

urls = ['https://www.naver.com/', 'https://www.python.org/']
filename = "robots.txt"

for url in urls:
    file_path = url + filename
    print(file_path)
    resp = requests.get(file_path)
    print(resp.text)
    print("\n")
~~~
~~~
<Output>
https://www.naver.com/robots.txt
User-agent: *
Disallow: /
Allow : /$ 



https://www.python.org/robots.txt
# Directions for robots.  See this URL:
# http://www.robotstxt.org/robotstxt.html
# for a description of the file format.

User-agent: HTTrack
User-agent: puf
User-agent: MSIECrawler
Disallow: /

# The Krugle web crawler (though based on Nutch) is OK.
User-agent: Krugle
Allow: /
Disallow: /~guido/orlijn/
Disallow: /webstats/

# No one should be crawling us with Nutch.
User-agent: Nutch
Disallow: /

# Hide old versions of the documentation and various large sets of files.
User-agent: *
Disallow: /~guido/orlijn/
Disallow: /webstats/
~~~

# 필기
~~~
- 로봇 배제 표준 : 웹 사이트에 로봇이 접근하는 것을 방지하기 위한 규약
- 일반적으로 접근 제한에 대한 설명을 robots.txt에 기술
- robots.txt 파일은 루트 디렉터리에 위치하므로 웹 페이지의 URL에 결합하여 웹 브라우저의 주소창에 입력하는 방식으로 확인 가능
  ex) http://www.python.org/robots.txt

~~~
